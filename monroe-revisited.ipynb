{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monroe County Revisited\n",
    "\n",
    "Originally an exercise at Lighthouse Labs, I wanted to come back to this to expand a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as re\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import JSON\n",
    "import time\n",
    "from geopy import distance\n",
    "#import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "The data can be found [here](https://drive.google.com/file/d/1_KF9oIJV8cB8i3ngA4JPOLWIE_ETE6CJ/view?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df = pd.read_csv(\"monroe-county-crash-data2003-to-2015.csv\", encoding=\"unicode_escape\")\n",
    "\n",
    "# preparing data\n",
    "accidents_df.dropna(subset=['Latitude', 'Longitude'], inplace=True)\n",
    "# creation of variable with lon and lat together\n",
    "accidents_df['ll'] = accidents_df['Latitude'].astype(str) + ',' + accidents_df['Longitude'].astype(str)\n",
    "# remove 0 lat and lon\n",
    "accidents_df = accidents_df[accidents_df['ll'] != '0.0,0.0']\n",
    "\n",
    "# rename columns\n",
    "accidents_df.columns =[column.replace(\" \", \"_\") for column in accidents_df.columns]\n",
    "print(accidents_df.shape)\n",
    "accidents_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make date from columns Year, Month, Day\n",
    "accidents_df['Date'] = pd.to_datetime(accidents_df[['Year', 'Month', 'Day']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute Weekend? from Date due to missing values\n",
    "accidents_df['Weekend'] = accidents_df['Date'].dt.dayofweek >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN Hour\n",
    "accidents_df.dropna(subset=['Hour'], inplace=True)\n",
    "# drop NaN Collision_Type\n",
    "accidents_df.dropna(subset=['Collision_Type'], inplace=True)\n",
    "# drop NaN Primary_Factor\n",
    "accidents_df.dropna(subset=['Primary_Factor'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format Hour from float to int\n",
    "accidents_df['Hour'] = accidents_df['Hour'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make timestamp column from accidents_df Date and Hour columns\n",
    "accidents_df['Timestamp'] = pd.to_datetime(accidents_df['Date'].astype(str) + accidents_df['Hour'].astype(str).str.zfill(4), format='%Y-%m-%d%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add feature night to indicate if it was dark\n",
    "accidents_df['Night'] = (accidents_df['Hour'] >= 18) | (accidents_df['Hour'] <= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only the columns we need\n",
    "accidents_df = accidents_df[['Timestamp', 'Weekend', 'Night', 'Collision_Type', 'Injury_Type', 'Primary_Factor', 'll']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show Primary_Factor types\n",
    "accidents_df['Primary_Factor'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NaN\n",
    "accidents_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Foursquare API\n",
    "\n",
    "Foursquare API documentation is [here](https://developer.foursquare.com/)\n",
    "\n",
    "1. Start a foursquare application and get your keys.\n",
    "2. For each crash, create the function **get_venues** that will pull bars in the radius of 5km around the crash\n",
    "\n",
    "#### example\n",
    "`get_venues('48.146394, 17.107969')`\n",
    "\n",
    "3. Find a relationship (if there is any) between number of bars in the area and severity of the crash.\n",
    "\n",
    "HINTs: \n",
    "- check out python package \"foursquare\" (no need to send HTTP requests directly with library `requests`)\n",
    "- **categoryId** for bars and nightlife needs to be found in the [foursquare API documentation](https://developer.foursquare.com/docs/api-reference/venues/search/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the keys\n",
    "foursquare_id = os.environ[\"FS_CLIENT_ID\"]\n",
    "foursquare_secret = os.environ[\"FS_CL_SECRET\"]\n",
    "foursquare_api = os.environ[\"FS_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize FS API\n",
    "headers = {\n",
    "\n",
    "    \"Accept\": \"application/json\",\n",
    "\n",
    "    \"Authorization\": foursquare_api\n",
    "\n",
    "}\n",
    "url=\"https://api.foursquare.com/v3/places/search\"\n",
    "radius = \"&radius=10000\"\n",
    "limit = \"&limit=50\"\n",
    "citycenter=\"39.1676747,-86.5314594\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fs_get_rect(northeast, southwest):\n",
    "    params = {\n",
    "\t\t\"query\": \"bar\",\n",
    "  \t\"ne\": northeast,\n",
    "  \t\"sw\": southwest\n",
    "\t}\n",
    "    response = re.request(\"GET\", url, params=params, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error:\", response.status_code)\n",
    "        return None\n",
    "    else:\n",
    "        barset = pd.DataFrame(columns=['name', 'lat', 'lng'])\n",
    "        data = response.json()\n",
    "        bars = data['results']\n",
    "        for bar in bars:\n",
    "            barset = barset.append({\n",
    "                'name': bar['name'],\n",
    "                'lat': bar['geocodes']['main']['latitude'],\n",
    "                'lng': bar['geocodes']['main']['longitude']\n",
    "            }, ignore_index=True)\n",
    "        return barset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test fs_get_rect\n",
    "resulttest = fs_get_rect(\"39.2525,-86.3656\", \"39.2400,-86.4656\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all bar locations from FS API using rectangular boundary\n",
    "def get_venues_loop(start_lat, start_lon, end_lat, end_lon):\n",
    "\tlat_point = start_lat\n",
    "\tlon_point = start_lon\n",
    "\tlat_step = 0.0292\n",
    "\tlon_step = 0.0155\n",
    "\tall_bars = pd.DataFrame(columns=['name', 'lat', 'lng'])\n",
    "\n",
    "\t#iterate through the rectangle\n",
    "\twhile lat_point > end_lat:\n",
    "\t\twhile lon_point > end_lon:\n",
    "\t\t\t# get the response from the FS API\n",
    "\t\t\tresponse = fs_get_rect(str(lat_point)+\",\"+str(lon_point), str(lat_point - lat_step)+\",\"+str(lon_point - lon_step))\n",
    "\t\t\tif response is not None:\n",
    "\t\t\t\tall_bars = all_bars.append(response, ignore_index=True)\n",
    "\t\t\t# sleep for 5 second to avoid rate limiting\n",
    "\t\t\tlon_point -= lon_step\n",
    "\t\t\ttime.sleep(5)\n",
    "\t\tlon_point = start_lon\n",
    "\t\tlat_point -= lat_step\n",
    "\treturn all_bars\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting coordinates is roughly top right corner of where the accidents are\n",
    "# end point is bottom left corner\n",
    "start_lat = 39.3525\n",
    "end_lat = 39.0425\n",
    "start_lon = -86.3656\n",
    "end_lon = -86.7067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = get_venues_loop(start_lat, start_lon, end_lat, end_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "bars.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ll column from lat and lng\n",
    "bars['ll'] = bars['lat'].astype(str) + ',' + bars['lng'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars.to_csv('bars.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = pd.read_csv('bars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df['closest_bar'] = np.nan\n",
    "accidents_df['number_of_bars_1km'] = np.nan\n",
    "accidents_df['number_of_bars_3km'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for each accident add a column with the closest bar distance and columns with amount of bars in 1km and 3km radius\n",
    "\n",
    "for acc_index, accident in accidents_df.iterrows():\n",
    "    closest_bar = 9000\n",
    "    number_of_bars_3 = 0\n",
    "    number_of_bars_1 = 0\n",
    "    for _, bar in bars.iterrows():\n",
    "        distance_from_bar = distance.distance(accident['ll'], bar['ll']).kilometers\n",
    "        if distance_from_bar < 1:\n",
    "            number_of_bars_1 += 1\n",
    "        if distance_from_bar < 3:\n",
    "            number_of_bars_3 += 1\n",
    "        if distance_from_bar < closest_bar:\n",
    "            closest_bar = distance_from_bar\n",
    "    accidents_df.loc[acc_index, 'closest_bar'] = closest_bar\n",
    "    accidents_df.loc[acc_index, 'number_of_bars_1km'] = number_of_bars_1\n",
    "    accidents_df.loc[acc_index, 'number_of_bars_3km'] = number_of_bars_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesonet ASOS weather data\n",
    "\n",
    "https://mesonet.agron.iastate.edu/ASOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv(\"BMG.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.rename(columns = {'valid': 'Time', 'sknt':'Wind_Speed', 'p01i':'Precipitation',\n",
    "                              'vsby':'Visibility', 'gust':'Wind_Gust', 'wxcodes': 'Weather Codes', 'ice_accretion_1hr': 'Ice_Accretion'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make timestamp out of 'Time' in weather_df\n",
    "weather_df['Time'] = pd.to_datetime(weather_df['Time'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather phenomena:\n",
    "RA Rain SN Snow SG Snow Grains\n",
    "DZ Drizzle IC Ice Crystals PL Ice pellets (sleet)\n",
    "GS Small hail GR Hail UP Unknown precipitation\n",
    "Obscurations to visibility:\n",
    "BR Mist (>=5/8 mi) FG Fog (< 5/8 mi)\n",
    "FU Smoke VA Volcanic Ash\n",
    "SA Sand HZ Haze\n",
    "PY Spray DU Widespread Dust\n",
    "Other:\n",
    "SQ Squall (strong wind) SS Sandstorm\n",
    "DS Duststorm PO Dust/sand whirls\n",
    "FC Funnel Cloud FC+ Tornado/waterspout\n",
    "Qualifiers (for RA, DZ, SN, PL):\n",
    "- Light\n",
    "(No sign) Moderate\n",
    "+ Heavy\n",
    "VC Vicinity\n",
    "Examples:\n",
    "+RA Heavy Rain\n",
    "-DZ Light Drizzle\n",
    "SN Moderate Snow\n",
    "VCTS Thunderstorm in the vicinity (5-10 mi from observation)\n",
    "Other Descriptors:\n",
    "MI Shallow BC Patches PR Partial\n",
    "TS Thunderstorm BL Blowing SH Showers\n",
    "DR Drifting FZ Freezing\n",
    "Examples:\n",
    "BCFG Patchy fog\n",
    "+TSRA Thunderstorm with heavy rain\n",
    "BLSN Blowing snow\n",
    "SHRA Moderate rain showers\n",
    "TSRAGR Thunderstorm with moderate rain and hail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append weather data to each accident. match closest timestamp\n",
    "for acc_index, accident in accidents_df.iterrows():\n",
    "    # get the closes timestamp in weather_df\n",
    "    closest_time = weather_df.iloc[(weather_df['Time']-accident['Timestamp']).abs().argsort()[:1]]\n",
    "\n",
    "    # append weather data to accident\n",
    "    accidents_df.loc[acc_index, 'Wind_Speed'] = closest_time['Wind_Speed'].values[0]\n",
    "    accidents_df.loc[acc_index, 'Precipitation'] = closest_time['Precipitation'].values[0]\n",
    "    accidents_df.loc[acc_index, 'Visibility'] = closest_time['Visibility'].values[0]\n",
    "    accidents_df.loc[acc_index, 'Wind_Gust'] = closest_time['Wind_Gust'].values[0]\n",
    "    accidents_df.loc[acc_index, 'Weather Codes'] = closest_time['Weather Codes'].values[0]\n",
    "    accidents_df.loc[acc_index, 'Ice_Accretion'] = closest_time['Ice_Accretion'].values[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df.to_csv(\"accidents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df = pd.read_csv(\"accidents.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode Injury_Type from 0 to 3\n",
    "accidents_df['Injury_Type'] = accidents_df['Injury_Type'].map({'No injury/unknown': 0, 'Non-incapacitating': 1, 'Incapacitating': 2, 'Fatal': 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df['Weather Codes'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some weather codes are combined. For training purposes we will split them using regex\n",
    "four_letters_regex = r\"\\w{4}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each 'Weather Codes' entry, split four letter codes in two\n",
    "accidents_df['Weather Codes'] = accidents_df['Weather Codes'].str.replace(four_letters_regex, lambda m: m.group(0)[:2] + ' ' + m.group(0)[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df['Weather Codes'] = accidents_df['Weather Codes'].str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode Weather Codes\n",
    "mlb = MultiLabelBinarizer()\n",
    "weather_codes_encoded = pd.DataFrame(mlb.fit_transform(accidents_df['Weather Codes']), columns=mlb.classes_, index=accidents_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_codes_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add weather codes to accidents_df\n",
    "accidents_df = pd.concat([accidents_df, weather_codes_encoded], axis=1)\n",
    "accidents_df.drop(columns=['Weather Codes'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode Collision_Type\n",
    "accidents_df = pd.get_dummies(accidents_df, columns=['Collision_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode Primary_Factor\n",
    "accidents_df = pd.get_dummies(accidents_df, columns=['Primary_Factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode Weekend?\n",
    "accidents_df = pd.get_dummies(accidents_df, columns=['Weekend?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df['Ice_Accretion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In 'Precipitation', replace M with 0 and T with 0.001\n",
    "accidents_df['Precipitation'] = accidents_df['Precipitation'].str.replace('M', '0')\n",
    "accidents_df['Precipitation'] = accidents_df['Precipitation'].str.replace('T', '0.0005')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In 'Wind_Speed', replace M with 0 and T with 0.001\n",
    "accidents_df['Wind_Speed'] = accidents_df['Wind_Speed'].str.replace('M', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In 'Visibility', replace M with 0\n",
    "accidents_df['Visibility'] = accidents_df['Visibility'].str.replace('M', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In 'Wind_Gust', replace M with 0\n",
    "accidents_df['Wind_Gust'] = accidents_df['Wind_Gust'].str.replace('M', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Ice_Accretion, replace M with 0 and T with 0.001\n",
    "accidents_df['Ice_Accretion'] = accidents_df['Ice_Accretion'].str.replace('M', '0')\n",
    "accidents_df['Ice_Accretion'] = accidents_df['Ice_Accretion'].str.replace('T', '0.0005')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Unnamed: 0' column that was created when saving to csv without index=False\n",
    "accidents_df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns not needed for training\n",
    "accidents_df.drop(columns=['Timestamp', 'll'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale 'number_of_bars_1km', 'number_of_bars_3km', 'closest_bar', 'Wind_Speed', 'Visibility' between 0 and 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "accidents_df[['number_of_bars_1km', 'number_of_bars_3km', 'closest_bar', 'Wind_Speed', 'Precipitation', 'Visibility', 'Wind_Gust', 'Ice_Accretion']] = scaler.fit_transform(accidents_df[['number_of_bars_1km', 'number_of_bars_3km', 'closest_bar', 'Wind_Speed', 'Precipitation', 'Visibility', 'Wind_Gust', 'Ice_Accretion']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = accidents_df.drop(columns=['Injury_Type'])\n",
    "y = accidents_df['Injury_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up param grid for logistic regression\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga', 'lbfgs']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up gridsearch for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit gridsearch\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show best parameters\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logistic regression model with best parameters\n",
    "clf = LogisticRegression(C=100, penalty='l2', solver='lbfgs', max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AOC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# predict probabilities\n",
    "probs = clf.predict_proba(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
